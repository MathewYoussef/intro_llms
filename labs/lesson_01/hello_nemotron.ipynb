{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae7a52-08c7-4fc5-a327-fcd6aae8d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hello Nemotron is the first lesson and the first part of the first lesson. It will involve running nemotron locally with transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e94b0-5df5-4aab-abed-5ec6d12a0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we ensure we can see the local cache where nemotron is stored (we are using the hugging face caching system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773914e7-4a04-45f2-af7f-1796b31ebafb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME = /data/hf\n",
      "/workspace/labs/lesson_01\n",
      "total 20\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Dec 29 05:36 .\n",
      "drwxr-xr-x 1 root   root   4096 Dec 29 12:04 ..\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4096 Dec 29 10:39 hf\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Nov 29 00:59 models\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Nov 29 05:35 vllm_cache\n",
      "total 32\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4096 Dec 29 10:39 .\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Dec 29 05:36 ..\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Dec 29 05:47 hub\n",
      "drwxr-xr-x 3 ubuntu ubuntu 4096 Dec 29 10:39 modules\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   60 Dec 29 05:46 stored_tokens\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   37 Dec 29 05:46 token\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4096 Dec 29 10:39 transformers\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Dec 29 05:47 xet\n",
      "total 32\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4096 Dec 29 10:39 .\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Dec 29 05:36 ..\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Dec 29 05:47 hub\n",
      "drwxr-xr-x 3 ubuntu ubuntu 4096 Dec 29 10:39 modules\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   60 Dec 29 05:46 stored_tokens\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   37 Dec 29 05:46 token\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4096 Dec 29 10:39 transformers\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Dec 29 05:47 xet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"HF_HOME =\", os.environ.get(\"HF_HOME\"))\n",
    "\n",
    "!pwd\n",
    "!ls -la /data | head\n",
    "!ls -la /data/hf | head\n",
    "!ls -la $HF_HOME | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699ea99-42f2-4330-ba4d-c737e0b8d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we ensure that transformers will load the model weights from the local Hugging Face Cache (HF Hub Cache) without redownlaoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a271e0fc-7087-451f-bd4d-ccc62e23ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME = /data/hf\n",
      "HF_HUB_CACHE = /data/hf/hub\n",
      "Snapshot dir: /data/hf/hub/models--nvidia--NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/snapshots/2e43387afd60157064e5bef4e9a583f887c6dfdd\n",
      "Shard count: 13\n",
      "First shard: /data/hf/hub/models--nvidia--NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/snapshots/2e43387afd60157064e5bef4e9a583f887c6dfdd/model-00001-of-00013.safetensors\n",
      "First shard is_symlink: True\n",
      "first shard -> ../../blobs/4c77b0f1717f1fb11791fb62fc57ca56f59fd1427ac466849ef9705ac90729ea\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    "\n",
    "print(\"HF_HOME =\", os.environ.get(\"HF_HOME\"))\n",
    "print(\"HF_HUB_CACHE =\", os.environ.get(\"HF_HUB_CACHE\"))\n",
    "\n",
    "#This forces an offline check: It will fail if the files arent already cached.\n",
    "snap_dir = snapshot_download(\n",
    "    repo_id=model_id,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "snap_path = Path(snap_dir)\n",
    "print(\"Snapshot dir:\", snap_path)\n",
    "\n",
    "# Prove the 13 shards are present in this snapshot.\n",
    "shards = sorted(snap_path.glob(\"model-*-of-00013.safetensors\"))\n",
    "print(\"Shard count:\", len(shards))\n",
    "print(\"First shard:\", shards[0] if shards else None)\n",
    "\n",
    "# Optional: show whether these are symlinks into blobs (typical HF cache layout)\n",
    "if shards:\n",
    "    print(\"First shard is_symlink:\", shards[0].is_symlink())\n",
    "    if shards[0].is_symlink():\n",
    "        print(\"first shard ->\", os.readlink(shards[0]))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cb8d8-5475-46be-b619-934559d2eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we load the tokenizer and model from the local HF cache and generate a short completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bad52c3-405b-4b26-a0f4-9a794fd5ce10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952f313b4e21434690d050bafcf7ea4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NemotronH requires an initialized `NemotronHHybridDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "user\n",
      "Hello Nemotron. In one sentence, who are you?\n",
      "assistant\n",
      "<think></think>I am Nemotron, a large language model created by NVIDIA to assist with reasoning, creativity, and problem-solving across a wide range of tasks.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    "revision = \"2e43387afd60157064e5bef4e9a583f887c6dfdd\"  # your cached snapshot\n",
    "cache_dir = \"/data/hf/hub\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,\n",
    "    revision=revision,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    revision=revision,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "messages = [{\"role\":\"user\",\"content\":\"Hello Nemotron. In one sentence, who are you?\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, enable_thinking=False,\n",
    "                                          add_generation_prompt=True,\n",
    "                                          return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=60, do_sample=False)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142469f-d588-40a2-88a8-bfdc98872787",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will now rerun with more verbosity to see if you can pick up more data about what the model is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6532bf4e-899d-4497-8aa0-867c6e362d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  We run a new model.generate() call using whatever input_ids were defined in the previous run (given that everything is still alive in the notebook kernel). Effectively, we are using the smae prompt as before. We first run generation and capture the result and the timing while it runs.  Then, we analyze the captured results (token coutns, decode, logprobs, etc)##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e692ad1-a7ba-4838-ba75-16af7882491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Nemotron, a large language model created by NVIDIA to assist with reasoning, creativity, and problem-solving across a wide range of tasks.\n",
      "prompt_tokens=28 new_tokens=31 time_s=27.629 tok_per_s=1.12 peak_cuda_mem_gb=58.94\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "\n",
    "# Reuse existing: `model`, `tokenizer`, `input_ids`\n",
    "eos = tokenizer.eos_token_id\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=False,\n",
    "        eos_token_id=eos,\n",
    "        pad_token_id=eos,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "seq = gen.sequences[0]\n",
    "prompt_tokens = input_ids.shape[-1]\n",
    "generated = seq[prompt_tokens:]\n",
    "\n",
    "# Trim at first EOS\n",
    "if eos is not None:\n",
    "    eos_pos = (generated == eos).nonzero(as_tuple=True)[0]\n",
    "    if eos_pos.numel() > 0:\n",
    "        generated = generated[: eos_pos[0]]\n",
    "\n",
    "text = tokenizer.decode(generated.detach().to(\"cpu\").tolist(),\n",
    "skip_special_tokens=True)\n",
    "\n",
    "new_tokens = int(generated.numel())\n",
    "dt = t1 - t0\n",
    "peak_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "print(text)\n",
    "print(f\"prompt_tokens={prompt_tokens} new_tokens={new_tokens} time_s={dt:.3f} tok_per_s={(new_tokens/dt if dt>0 else float('inf')):.2f} peak_cuda_mem_gb={peak_gb:.2f}\")\n",
    "\n",
    "# Top-5 alternatives for the first generated token\n",
    "if gen.scores:\n",
    "    logits0 = gen.scores[0][0].float()\n",
    "    probs0 = torch.softmax(logits0, dim=-1)\n",
    "    top = torch.topk(probs0, k=5)\n",
    "    ids = top.indices.tolist()\n",
    "    ps = top.values.tolist()\n",
    "    toks = [tokenizer.decode([i], skip_special_tokens=False) for i in ids]\n",
    "    print(\"top5_first_token:\", list(zip(ids, ps, toks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec5175e7-1df9-411d-b376-8918e26294c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.logits.device = cuda:0\n",
      "gen.scores[0].device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "print(\"outputs.logits.device =\", outputs.logits.device)\n",
    "\n",
    "if \"gen\" in globals() and getattr(gen, \"scores\", None):\n",
    "    print(\"gen.scores[0].device =\", gen.scores[0].device)\n",
    "else:\n",
    "    print(\"gen.scores not available (run generate with output_scores=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "debdd5ad-02eb-4a11-91ae-57227220649c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moutputs\u001b[49m.logits.device\n",
      "\u001b[31mNameError\u001b[39m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f38ef054-28b6-47fe-b413-302f119d0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(out) = <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n",
      "has sequences = True\n",
      "seq dtype/device/shape = torch.int64 cuda:0 (60,)\n",
      "seq min/max = 0 4472591304561587027\n",
      "bad ids count = 30\n",
      "first bad ids = [4454694472195620972, 4472591304561587027, 4470358711844355279, 4438422632114747494, 4439308907195912046, 4442371755751307659, 4438863235618130594, 4442021299304623625, 4438912799541253756, 4441276582037614266, 4458430367888893160, 4464418449943726864, 4451499798209831319, 4432637246734100849, 4429952767616152071, 4453393556659185601, 4438361892677319790, 4452046182466844564, 4455312754200068214, 4435918799314393832]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d52b2b-34f9-454b-97dc-650adef94355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
